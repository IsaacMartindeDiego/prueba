---
title: "Practica_MD"
author: "Grupo de Prueba"
date: "`r format(Sys.Date(), '%d de %B de %Y')`"
output: bookdown::gitbook
---

# Introducción

Bla Bla Bla

Cargamos las librerías que vamos a necesitar para esta práctica.
```{r librerías}
library(ggplot2) # Gráficos
```

# Objetivos (del negocio)

Bla Bla Bla

# Datos 

Leemos los datos y comprobamos el tamaño de la base de datos.

```{r lectura de datos}
diabetes <- read.csv("~/data/DSLab/Cursos/Data Mining /2018 ERICSSON_SEPT/14. Ejercicios Prácticos/diabetes.csv")
dim(diabetes)
head(diabetes)
summary(diabetes)
```
Podemos observar cómo la variable respuesta *outcome* es continua cuando debería de ser una variable binaria. En el siguiente capítulo realizaremos un análisis exploratorio de los datos que nos servirá para estudiar el comportamiento de las variables y, en caso de ser necesario, transformar dichas variables.

Las variables y su explicación aparecen en la siguiente lista:

Nombre | Descripción
-------| -----------
Pregnancies | bla bla bla
Glucose | bla bla bla
Outcome | variable respuesta, 1 si es diabetes igual a sí.

## Particiones

```{r particiones}
n_total = dim(diabetes)[1]
n_train = floor(0.6 * n_total) # el 60% de la muestra se usa para entrenar los modelos
n_test = floor(0.2 * n_total) # el 20% para test
n_validation = n_total - n_train - n_test # el resto para validación

indices_total = seq(1:n_total)
set.seed(233834)
indices_train = sample(indices_total, n_train)
indices_test = sample(indices_total[-indices_train],n_test)
indices_validation=indices_total[-c(indices_train,indices_test)]

diabetes_train=diabetes[indices_train,]
diabetes_test = diabetes[indices_test,]
diabetes_validation = diabetes[indices_validation,]

```

# Preprocesado de datos

Bla Bla Bla

## EDA. Exploratory Data Analysis.

En primer lugar, corregimos la variable salida "Outcome", convirtiéndola en binaria (factor).

```{r binarización}
diabetes$Outcome=as.factor(diabetes$Outcome)
```

```{r EDA}
attach(diabetes_train)
summary(Pregnancies)
table(Pregnancies)
ggplot(data=diabetes,aes(Pregnancies))+
  geom_density()+
  ggtitle("Función de Densidad de la variable Pregnancies")
```

## Correlación entre las variables.

Estudiamos la correlación entre las variables explicativas y la variable respuesta.

### Pregnancies:

```{r Corr, echo=FALSE}
ggplot(data=diabetes, aes(x=Pregnancies, fill=Outcome)) + geom_density(alpha=.3)
```
Realizamos un contraste de hipótesis (t.test) para comprobar si hay diferencias significativas entre los dos grupos de interés:

```{r TEST_PREGNANCIES}
t.test(Pregnancies~Outcome)
```
Se puede concluir que la variable *Pregnancies* es importante para determinar si una mujer va a ser diabética o no.

## Transformación de variables
Bla bla bla

```{r Skin}
hist(SkinThickness)
```

Aparentemente hay valores 0 que corresponden con mujeres a las que no se les tomó esta medida. 

```{r Skin_transformation}
Skin_cat = cut(SkinThickness,c(-1,0.1,20,40,1000),labels=c("NS/NC","Bajo","Medio","Alto"))
table(Skin_cat,Outcome)
chisq.test(table(Skin_cat,Outcome)
)
```
Hemos transformado la variable, categorizándola en 4 niveles. La nueva variable resulta ser estadísticamente significativa. Es decir, es una variable que debemos tener en cuenta en la construcción de los modelos de Machine Learning (ML).

## Imputacion de datos (datos faltantes)

# Modelado

## k-NN
## Regresión Logística
## Clustering
## Arboles de Decisión
## Bosques Aleatorios
## Support Vector Machines
## Redes Neuronales

# Evaluación

# Visualización y Presentación de Resultados

# Puesta en producción

# Actualización del modelo...
Vuelta a empezar.





